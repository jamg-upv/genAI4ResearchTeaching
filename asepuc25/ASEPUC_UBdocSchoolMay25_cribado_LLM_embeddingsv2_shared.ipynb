{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "r2dZicdd5P7G",
        "-UO5F7yQ5bV5",
        "k4fyV1eh8uzx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamg-upv/genAI4ResearchTeaching/blob/main/asepuc25/ASEPUC_UBdocSchoolMay25_cribado_LLM_embeddingsv2_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este codigo funciona con un entorno de de computacion de 12GB RAM sin GPU.\n",
        "Lo he probado con 4 categorias y 1300 abstracts a clasificar y con modelos de embeedings de 384 tokens no gasta más de 3Gb de ram. Con los 14 modelos (donde hay algunos de 1024 tokens) llega a gastar 6Gb (la ram depende de los tokens del modelo y no de la cantidad de objetos a embedd)\n",
        "los modelos de 384 tokens consumen unos 1-3 minutos por cada 600 abstracts\n",
        "Los modelosd e 720 tokens consumen unos 5-15 minutos por cada 600 abstracts\n"
      ],
      "metadata": {
        "id": "spIdvUXwcWiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vZ_ge4PTVeie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparacion previa (ejecutar solo una vez)\n"
      ],
      "metadata": {
        "id": "X8FGO3RO7rWU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7BUJlZ-keFcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## (solo ejecutar si tus datos están en github y quieres importarlos) conexion a github"
      ],
      "metadata": {
        "id": "r2dZicdd5P7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#conectar con repositorio github publico para acceso a los datasets (ajusta el nombre del repositorio al que corresponda)\n",
        "!git clone https://github.com/jamg-upv/LLMforSLRscreening.git\n",
        "\n",
        "# elije la carpeta del repositorio donde esta el archivo de datos   ACEDE-ECNwsDic24/\n",
        "# input_path = '/content/LLMforSLRscreening/datasets/'\n",
        "input_path = '/content/LLMforSLRscreening/ACEDE-ECNwsDic24/'\n",
        "\n",
        "# Define el nombre del archivo #Receuerda: estructura del archivo de datos:\n",
        "# Component\tDescription\tType\tClass\n",
        "input_file_name = 'ACEDE-ECN-workshop-dic-2024-HIWP3.csv'\n",
        "\n",
        "\n",
        "\n",
        "output_path = '/content/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD9kl1-cn-AW",
        "outputId": "ef22c4c9-e2ec-4194-90d9-b34d51a4dc6c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLMforSLRscreening'...\n",
            "remote: Enumerating objects: 460, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 460 (delta 93), reused 20 (delta 20), pack-reused 317 (from 2)\u001b[K\n",
            "Receiving objects: 100% (460/460), 42.89 MiB | 11.71 MiB/s, done.\n",
            "Resolving deltas: 100% (230/230), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Solo ejecutar si quieres guardar los resultados en TU google drive)  googledrive montado en local"
      ],
      "metadata": {
        "id": "-UO5F7yQ5bV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#conecar a google drive para guardar alli los outputs que quiera mantener (y que no se me olvide descargarlos) luego los subiré a Git para su uso posterior\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Montar Google Drive si aún no está montado\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define el path de salida en Google Drive\n",
        "output_path = '/content/drive/MyDrive/Reto21dias_24/'\n",
        "\n",
        "# Verifica si el directorio existe y si no, lo crea\n",
        "if not os.path.exists(output_path):\n",
        "    try:\n",
        "        os.makedirs(output_path)\n",
        "        print(f\"Directorio creado: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al crear el directorio: {e}\")\n",
        "else:\n",
        "    print(f\"El directorio ya existe: {output_path}\")"
      ],
      "metadata": {
        "id": "SdqRhEZQ5jUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b5f8fea8-2345-4f08-9c2e-3d375673f45d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2511543883.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Montar Google Drive si aún no está montado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Define el path de salida en Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ruta a los archivos cargados y guardados en raiz de Colab (si se corta la conexion sin descargarlos se pierden)"
      ],
      "metadata": {
        "id": "2nRmVEZd5n8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define el nombre del archivo y el path de entrada y salida en raiz de google colab\n",
        "\n",
        "# estructura del archivo de datos input_file: # Component\tDescription\tType\tClass\n",
        "# input_file_name = 'ACEDE-ECN-workshop-dic-2024-HIWP3.csv'\n",
        "# input_file_name = 'ACEDE-ECN-workshop-dic-2024-HIWP.xlsx'\n",
        "input_file_name = 'UBschoolMay25-dataCribadoTransformersNewCat.xlsx'\n",
        "\n",
        "input_path = '/content/'\n",
        "\n",
        "output_path = '/content/'"
      ],
      "metadata": {
        "id": "TLocOZAc5xe2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parametrización de variables\n"
      ],
      "metadata": {
        "id": "xvXuHnNnSZZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os # Import the os module at the top of your script\n",
        "# Construye el path completo\n",
        "full_in_path = os.path.join(input_path, input_file_name)\n",
        "full_in_path\n",
        "# full_out_path = os.path.join(output_path, file_name)\n",
        "\n",
        "# Define la variable con el nombre de la columna\n",
        "target_column = 'Description'\n",
        "\n",
        "# Objetos a clasificar\n",
        "selec_objects = ['art1','art']\n",
        "# selec_objects = ['cat1']\n",
        "\n",
        "#categorias en la que quiero clasificar los objetos\n",
        "# selec_categories = ['cat3']\n",
        "selec_categories = ['cat1', 'cat2']\n",
        "\n",
        "# el identificador [component] de la categoria que quiero mostrar en el resumen de clasificación global\n",
        "\n",
        "# target_category ='promptB'\n",
        "target_category ='SLR'\n"
      ],
      "metadata": {
        "id": "WVOoxtMCSpux"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prueba (en desarrollo) para leer diferentes formatos de csv (no comprobada)"
      ],
      "metadata": {
        "id": "k4fyV1eh8uzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def leer_archivo(ruta_archivo, **kwargs):\n",
        "    extension = ruta_archivo.split('.')[-1].lower()\n",
        "\n",
        "    try:\n",
        "        if extension in ['xlsx', 'xls']:\n",
        "            return pd.read_excel(ruta_archivo, **kwargs)\n",
        "        elif extension == 'csv':\n",
        "            # Lista de configuraciones a intentar\n",
        "            configs = [\n",
        "                {'encoding': 'utf-8', 'sep': ','},\n",
        "                {'encoding': 'latin1', 'sep': ';'},\n",
        "                {'encoding': 'utf-8', 'sep': ';'},\n",
        "                {'encoding': 'cp1252', 'sep': ';'}  # Común en Windows\n",
        "            ]\n",
        "\n",
        "            # Intentar cada configuración\n",
        "            for config in configs:\n",
        "                try:\n",
        "                    # Combinar configuración predeterminada con kwargs\n",
        "                    params = {**config, **kwargs}\n",
        "                    return pd.read_csv(ruta_archivo, **params)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            raise ValueError(\"No se pudo leer el archivo CSV con ninguna configuración\")\n",
        "        else:\n",
        "            raise ValueError(f\"Formato de archivo no soportado: {extension}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Ejemplos de uso:\n",
        "# Lectura básica\n",
        "hiwp_data = leer_archivo(full_in_path)\n",
        "\n",
        "# Lectura con parámetros específicos\n",
        "# hiwp_data = leer_archivo(full_in_path, decimal=',', thousands='.')\n",
        "# hiwp_data = leer_archivo(full_in_path, sheet_name='Hoja1')  # Para Excel"
      ],
      "metadata": {
        "id": "Es1B4cfi7zBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# leer el archivo"
      ],
      "metadata": {
        "id": "UzfLid5W9DZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Leer el archivo CSV o EXCEL\n",
        "# Originalmente era un CSV de Open Office porque si lo generaba con MSexcel no se mapeaba bien )\n",
        "    # Para guardar correctamente un archivo CSV con comas desde OpenOffice cuando viene de Excel con punto y coma (;), sigue estos pasos:\n",
        "    # En OpenOffice, ve a \"Archivo → Guardar como\"\n",
        "    # Selecciona \"Texto CSV (.csv)\" como tipo de archivo\n",
        "    # Marca la casilla \"Editar configuración de filtro\"\n",
        "    # En la ventana de configuración:\n",
        "    # Juego de caracteres: Unicode (UTF-8)\n",
        "    # Delimitador de campo: selecciona \",\" (coma)\n",
        "    # Delimitador de texto: \" (comillas)\n",
        "    # NO se marca tener todas las cadenas de texto con \"\"\n",
        "#mejoro el codigo con esta funcion que debe gestionar automático el formato de CSV o de excel (la primera de las hojas del libro)\n",
        "\n",
        "def leer_archivo(ruta_archivo):\n",
        "    # Obtener la extensión del archivo\n",
        "    extension = ruta_archivo.split('.')[-1].lower()\n",
        "\n",
        "    try:\n",
        "        if extension in ['xlsx', 'xls']:\n",
        "            # Leer archivo Excel\n",
        "            return pd.read_excel(ruta_archivo)\n",
        "        elif extension == 'csv':\n",
        "            # Intentar primero con encoding UTF-8 y separador ','\n",
        "            try:\n",
        "                return pd.read_csv(ruta_archivo, encoding='utf-8')\n",
        "            except:\n",
        "                # Si falla, intentar con encoding latin1 y separador ';'\n",
        "                try:\n",
        "                    return pd.read_csv(ruta_archivo, encoding='latin1', sep=';')\n",
        "                except:\n",
        "                    # Último intento con encoding utf-8 y separador ';'\n",
        "                    return pd.read_csv(ruta_archivo, encoding='utf-8', sep=';')\n",
        "        else:\n",
        "            raise ValueError(f\"Formato de archivo no soportado: {extension}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Uso de la funcion\n",
        "hiwp_data = leer_archivo(full_in_path)\n",
        "\n",
        "# hiwp_data = leer_archivo('/content/ART-749-dataCribadoTransformers.xlsx')\n",
        "# hiwp_data = pd.read_excel('/content/ART-749-dataCribadoTransformers.xlsx')\n",
        "hiwp_data\n",
        "# Verificar si la lectura fue exitosa\n",
        "if hiwp_data is not None:\n",
        "    print(\"Archivo leído correctamente\")\n",
        "    print(f\"Dimensiones del DataFrame: {hiwp_data.shape}\")\n",
        "else:\n",
        "    print(\"No se pudo leer el archivo\")\n",
        "\n",
        "# Mostrar el DataFrame resultante\n",
        "# print(hiwp_data)\n",
        "\n",
        "# Crear el DataFrame 'objects'\n",
        "objects = hiwp_data[hiwp_data['Type'].isin(selec_objects)]\n",
        "\n",
        "# Crear el DataFrame 'categories'\n",
        "categories = hiwp_data[hiwp_data['Type'].isin(selec_categories)]\n",
        "\n",
        "# Opción 1: Reconstruir el índice descartando el índice original\n",
        "# objects= objects.reset_index(drop=True)\n",
        "# categories= categories.reset_index(drop=True)\n",
        "\n",
        "# Opción 2: Reconstruir el índice manteniendo el índice original como una nueva columna\n",
        "objects= objects.reset_index()\n",
        "categories= categories.reset_index()\n",
        "\n",
        "\n",
        "# Verificar los resultados\n",
        "print(\"DataFrame 'objects':\")\n",
        "print(objects.shape)\n",
        "print(objects['Type'].value_counts())\n",
        "print(\"\\nPrimeras filas de 'objects':\")\n",
        "print(objects.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"DataFrame 'categories':\")\n",
        "print(categories.shape)\n",
        "print(categories['Type'].value_counts())\n",
        "print(\"\\nPrimeras filas de 'categories':\")\n",
        "print(categories.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkStor1tSnr3",
        "outputId": "f086fda4-b916-49b3-ef23-87caea5aa72d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo leído correctamente\n",
            "Dimensiones del DataFrame: (961, 3)\n",
            "DataFrame 'objects':\n",
            "(958, 4)\n",
            "Type\n",
            "art    958\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Primeras filas de 'objects':\n",
            "   index                               Component  \\\n",
            "0      3        id0001--10.1145/3322134.3322138#   \n",
            "1      4                               id0002--#   \n",
            "2      5     id0004--10.2753/CED1061-1932420212#   \n",
            "3      6  id0006--10.1080/02602938.2021.1909702#   \n",
            "4      7     id0008--10.3389/fpsyg.2022.1004487#   \n",
            "\n",
            "                                         Description Type  \n",
            "0  Study on Data Analysis of Assessment in Class ...  art  \n",
            "1  Development and Piloting of Online Student Eva...  art  \n",
            "2  On Student Evaluation of Teaching and Improvem...  art  \n",
            "3  The student evaluation of teaching and likabil...  art  \n",
            "4  A literature review of the research on student...  art  \n",
            "\n",
            "==================================================\n",
            "\n",
            "DataFrame 'categories':\n",
            "(3, 4)\n",
            "Type\n",
            "cat2    2\n",
            "cat1    1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Primeras filas de 'categories':\n",
            "   index  Component                                        Description  Type\n",
            "0      0        SLR  A systematic literature review (SLR) is a meth...  cat2\n",
            "1      1        SET  Student Evaluation of Teaching (SET) refers to...  cat1\n",
            "2      2  OpenEnded  Open-ended questions are a form of interrogati...  cat2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Embeedings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Djbdr5_wJvRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib import metadata\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "required_packages = [\n",
        "    'seaborn',\n",
        "    'matplotlib',\n",
        "    'sentence-transformers',\n",
        "    'pandas',\n",
        "    'numpy',\n",
        "    'transformers',\n",
        "    'torch',\n",
        "    'xlrd',\n",
        "    'openpyxl'\n",
        "]\n",
        "\n",
        "def get_installed_packages():\n",
        "    \"\"\"Obtiene una lista de paquetes instalados usando importlib.metadata\"\"\"\n",
        "    try:\n",
        "        return {dist.metadata['Name'].lower() for dist in metadata.distributions()}\n",
        "    except Exception:\n",
        "        return set()\n",
        "\n",
        "def install_packages():\n",
        "    installed_packages = get_installed_packages()\n",
        "    packages_to_install = [\n",
        "        pkg for pkg in required_packages\n",
        "        if pkg.lower().replace('-', '_') not in installed_packages\n",
        "    ]\n",
        "\n",
        "    if packages_to_install:\n",
        "        print(\"Instalando paquetes faltantes...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages_to_install)\n",
        "            print(\"\\nTodos los paquetes faltantes han sido instalados.\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"\\nError durante la instalación: {e}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(\"Todos los paquetes requeridos ya están instalados.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def verify_installations():\n",
        "    print(\"\\nVerificando instalaciones:\")\n",
        "    all_installed = True\n",
        "    for package in required_packages:\n",
        "        try:\n",
        "            # Intentar importar el módulo\n",
        "            importlib.import_module(package.replace('-', '_'))\n",
        "            # Obtener la versión instalada\n",
        "            version = metadata.version(package)\n",
        "            print(f\"✓ {package} (versión {version})\")\n",
        "        except (ImportError, metadata.PackageNotFoundError):\n",
        "            print(f\"✗ {package} - No se pudo importar\")\n",
        "            all_installed = False\n",
        "    return all_installed\n",
        "\n",
        "def load_required_imports():\n",
        "    \"\"\"Función separada para cargar las importaciones necesarias\"\"\"\n",
        "    try:\n",
        "        # Realizar las importaciones globalmente\n",
        "        global SentenceTransformer, util, pd, np, torch, time\n",
        "        from sentence_transformers import SentenceTransformer, util\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        import torch\n",
        "        import time\n",
        "        print(\"\\nTodas las importaciones completadas con éxito.\")\n",
        "        return True\n",
        "    except ImportError as e:\n",
        "        print(f\"\\nError al importar las bibliotecas: {e}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    # Ejecutar instalación y verificación\n",
        "    if install_packages() and verify_installations():\n",
        "        print(\"\\nProcediendo con las importaciones...\")\n",
        "        return load_required_imports()\n",
        "    else:\n",
        "        print(\"\\nHubo problemas con la instalación o verificación de paquetes.\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    if success:\n",
        "        # Aquí ya puedes usar SentenceTransformer y demás imports\n",
        "        print(\"Sistema listo para usar\")"
      ],
      "metadata": {
        "id": "irPyxNC-_8uw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5524a1c-41ea-403c-c314-58ed463f72d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando paquetes faltantes...\n",
            "\n",
            "Todos los paquetes faltantes han sido instalados.\n",
            "\n",
            "Verificando instalaciones:\n",
            "✓ seaborn (versión 0.13.2)\n",
            "✓ matplotlib (versión 3.10.0)\n",
            "✓ sentence-transformers (versión 4.1.0)\n",
            "✓ pandas (versión 2.2.2)\n",
            "✓ numpy (versión 2.0.2)\n",
            "✓ transformers (versión 4.53.0)\n",
            "✓ torch (versión 2.6.0+cu124)\n",
            "✓ xlrd (versión 2.0.2)\n",
            "✓ openpyxl (versión 3.1.5)\n",
            "\n",
            "Procediendo con las importaciones...\n",
            "\n",
            "Todas las importaciones completadas con éxito.\n",
            "Sistema listo para usar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "# Obtener la fecha y hora actual para etiquear hora del analisis en los nombres de archivo\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
      ],
      "metadata": {
        "id": "bCdIE0naMMCH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de modelos a probar\n",
        "# Seleccion de los 1 de los rapidos entre los 5 mejores del articulo WPOM 2024\n",
        "# models_to_test = [\n",
        "#     'all-MiniLM-L6-v2'\n",
        "# ]\n",
        "\n",
        "# todos los modelos\n",
        "# models_to_test = [\n",
        "#     'all-MiniLM-L6-v2',\n",
        "#     'all-distilroberta-v1',\n",
        "#     'all-mpnet-base-v2',\n",
        "#     'paraphrase-multilingual-mpnet-base-v2',\n",
        "#     'distiluse-base-multilingual-cased-v1',\n",
        "#     'all-MiniLM-L12-v2',\n",
        "#     'allenai-specter',\n",
        "#     'allenai/scibert_scivocab_uncased',\n",
        "#     'distilbert-base-nli-mean-tokens',\n",
        "#     'roberta-base-nli-stsb-mean-tokens',\n",
        "#     'distiluse-base-multilingual-cased-v2',\n",
        "#     'paraphrase-multilingual-MiniLM-L12-v2',\n",
        "#     'stsb-roberta-large',\n",
        "#     'bert-base-nli-mean-tokens'\n",
        "#     'BAAI/bge-large-en-v1.5',    # 1024D - Top choice\n",
        "#     'intfloat/e5-large-v2',      # 1024D - Alternativa sólida\n",
        "#     'sentence-transformers/gtr-t5-large',      # Bueno para académicos\n",
        "#     'allenai/specter2_proximity' #ideal para cribado de revisiones sistemáticas de literatura (SLR), ya que está específicamente diseñado para trabajar con documentos científicos.\n",
        "# ]\n",
        "\n",
        "# Seleccion de los 5 mejores del articulo WPOM 2024\n",
        "models_to_test = [\n",
        "    'all-MiniLM-L6-v2'\n",
        "    # 'all-distilroberta-v1',\n",
        "    # 'all-mpnet-base-v2',\n",
        "    # 'all-MiniLM-L12-v2',\n",
        "    # 'allenai-specter',\n",
        "    # 'BAAI/bge-large-en-v1.5',    # 1024D - Top choice\n",
        "    #  'allenai/specter2_proximity' # pendiente de parametrizar para que funcione specter ideal para cribado de revisiones sistemáticas de literatura (SLR), ya que está específicamente diseñado para trabajar con documentos científicos.\n",
        "\n",
        "]\n",
        "\n",
        "# Seleccion de los peores del articulo WPOM 2024\n",
        "# models_to_test = [\n",
        "#     'paraphrase-multilingual-mpnet-base-v2',\n",
        "#     'distiluse-base-multilingual-cased-v1',\n",
        "#     'allenai/scibert_scivocab_uncased',\n",
        "#     'distilbert-base-nli-mean-tokens',\n",
        "#     'roberta-base-nli-stsb-mean-tokens',\n",
        "#     'distiluse-base-multilingual-cased-v2',\n",
        "#     'paraphrase-multilingual-MiniLM-L12-v2',\n",
        "#     'stsb-roberta-large',\n",
        "#     'bert-base-nli-mean-tokens'\n",
        "# ]\n",
        "\n",
        "# Función para evaluar un modelo\n",
        "def evaluate_model(model_name, objects_df, categories_df):\n",
        "    print(f\"Evaluating model: {model_name}\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generar embeddings para objetos\n",
        "    object_texts = objects_df[target_column].tolist()\n",
        "    object_embeddings = model.encode(object_texts, show_progress_bar=True)\n",
        "\n",
        "    # Generar embeddings para categorías\n",
        "    category_texts = categories_df[target_column].tolist()\n",
        "    category_embeddings = model.encode(category_texts, show_progress_bar=True)\n",
        "\n",
        "    # Calcular similitud coseno\n",
        "    similarity_matrix = util.cos_sim(object_embeddings, category_embeddings)\n",
        "\n",
        "    # Encontrar las mejores coincidencias\n",
        "    best_matches = np.argmax(similarity_matrix, axis=1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'best_matches': categories_df.iloc[best_matches]['Component'].tolist(),\n",
        "        'processing_time': round(processing_time, 2),  # Redondear a 2 decimales\n",
        "        'embedding_size': object_embeddings.shape[1],\n",
        "        'similarity_matrix': similarity_matrix,\n",
        "        'object_embeddings': object_embeddings,\n",
        "        'category_embeddings': category_embeddings\n",
        "    }\n",
        "\n",
        "# Evaluar todos los modelos\n",
        "results = []\n",
        "similarity_data = []\n",
        "embeddings_data = []\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    try:\n",
        "        result = evaluate_model(model_name, objects, categories)\n",
        "        results.append({\n",
        "            'model': result['model'],\n",
        "            'best_matches': result['best_matches'],\n",
        "            'processing_time': result['processing_time'],\n",
        "            'embedding_size': result['embedding_size']\n",
        "        })\n",
        "\n",
        "        # Preparar datos de similitud para este modelo\n",
        "        for i, obj in enumerate(objects['Component']):\n",
        "            row = [model_name, obj] + result['similarity_matrix'][i].tolist()\n",
        "            similarity_data.append(row)\n",
        "\n",
        "        # Preparar datos de embeddings para este modelo\n",
        "        for i, obj in enumerate(objects['Description']):\n",
        "            embeddings_data.append([model_name, objects['Component'].iloc[i], obj, result['object_embeddings'][i].tolist()])\n",
        "        for i, cat in enumerate(categories['Description']):\n",
        "            embeddings_data.append([model_name, categories['Component'].iloc[i], cat, result['category_embeddings'][i].tolist()])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {model_name}: {str(e)}\")\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results = df_results.rename(columns={'processing_time': 'processing_time_seconds'})\n",
        "print(df_results)\n",
        "\n",
        "# Crear DataFrame de similitud coseno\n",
        "columns = ['Model', 'Object'] + categories['Component'].tolist()\n",
        "df_similarity = pd.DataFrame(similarity_data, columns=columns)\n",
        "\n",
        "# Crear DataFrame de embeddings\n",
        "df_embeddings = pd.DataFrame(embeddings_data, columns=['Model', 'Component', 'Description', 'Embedding'])\n",
        "\n",
        "# Crear la nueva tabla de similitud para HIWPshortDescription\n",
        "df_hiwp_similarity = df_similarity[['Model', 'Object', target_category]].copy()\n",
        "df_hiwp_similarity.columns = ['Model', 'Object_Description', 'Similarity']\n",
        "df_hiwp_similarity['Object_Component'] = objects['Component'].tolist() * len(models_to_test)\n",
        "df_hiwp_similarity = df_hiwp_similarity[['Model', 'Object_Component', 'Object_Description', 'Similarity']]\n",
        "df_hiwp_similarity = df_hiwp_similarity.sort_values(['Model', 'Similarity'], ascending=[True, False])\n"
      ],
      "metadata": {
        "id": "52Ci32ItW0xd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "ff4e0766705547a3bd50949f7b6e76e3",
            "14173568dd4c4d23b5670d11325e617a",
            "79ae1ed330664c1baa873a2ab3994b76",
            "612e573327174b5499f93ca6bd35493b",
            "f2265ea8c20140bc858032b34debdb77",
            "a4deb208948a4509a5fd1baae1acc086",
            "4953f94b8c534de6a8d85c47bb5f865b",
            "12362eb189cc42d691be14ea834710f5",
            "36638bb5aac34bbebaf266edc99d6f6e",
            "f6166dd50f9b41bf841b6d82d9f58b2c",
            "f18d7becf4ac4c6293d782f4a7d3da97",
            "5fe5620b60304bbc9359aa500d30d365",
            "d00f77541e284e5ea85a4e6411e8415c",
            "c91730894b004d1b9e5467cc658a3520",
            "8b2f61e158764fe1a286b943dbf17e88",
            "56544f4439b34c678ebca7295804b1f3",
            "37e3d59ae7d64ce5a0c15acd4a34c108",
            "8467852144c642eca28bf590e815a8e4",
            "87e318146c45405c98d37900d0e20a10",
            "09be4ff6766d4e9f9e175bcd4b8150eb",
            "730b1756b55c417ab125ae6051187f0c",
            "18c3450d008c4387bb53b3cdaf450a52",
            "33aefd46a1e14da1815db0fa8579d621",
            "bffca2038c424a689679c912c6a3add9",
            "bb10ebde2e9b4ee99311fc29d230d2e0",
            "b7ba3131d9c64df383a53e4a657d11cd",
            "a17ef641d6f242f8b862898446883aa8",
            "486deee29644469abc292d2be230c514",
            "3d48886b1d8944cf85f4776cb29dd29a",
            "d1c5e9f6ce9447918721285a07cb1f3a",
            "d82f32af8a124b279983a9c27a23d97e",
            "976cf4b33aee4100948db4a565288fc4",
            "66552589d62b430880b1f59893e494b4",
            "c1ed57d08a6548639b17497df95a9949",
            "d4e6cd0ccef049508398e688449164bc",
            "cd81269ab558404195fae92c4528fa99",
            "08a58ef3bcab4edab86858c07e3983ec",
            "fc68a215fdc941e196f2dd4bb068e9c8",
            "14f7f9ab188246779feb1889180bce5f",
            "8014bdb567864eddb4c3f1244c653d12",
            "9f0a92fd49cb4b518216ba080fbf26a4",
            "958c4d99236145a39f16e8765388ff01",
            "833539a1cb4f4eb4a9d10d4c03606978",
            "873691a9fd184c46b58077ac24d37cdc",
            "a6eeae078508403c80b74eccb8617ef6",
            "3163fbc11f38469bb6b6eb10369c89a3",
            "86b4438560494e8eb9dec9f6326826db",
            "a2d42c2d8be748a88e5ed0c70e6eec8e",
            "cc2f48b4d0f043518652b1f504ba0481",
            "b9edab0be0e04a1ab26d25aae3dee842",
            "5a586e908b284c279c5cb0739947d9f5",
            "df97180c6eea4aac968295ddf484b0e5",
            "eb3f90b01af4447b91bb5e1241dd969f",
            "cef829238d8b4f50bcb7c6ed3f7e0e3d",
            "90e2cea068914c038c79c7d463d70920",
            "2507e138792a40f59ac80d1eb123c836",
            "33f2e8aa15c14d909da1f9b17d73a10d",
            "131750e4748c4f329ad96e35f877bc21",
            "df368d9f749d4df5ae6c702dc8d7ba71",
            "76931eacd90f4e30939b907d8d0d0328",
            "dd910d4a917842d086ca69c6e18ddf0f",
            "aaab3389c2a44b6d9c0a35783b32649b",
            "b39526d49af84609a0c976b343bf3e60",
            "24c32cf7fa974745b26129db43afeeb4",
            "efb57f153fc94245b422d9649f64796c",
            "4085aa3a752148c69c471c07d1e1d14f",
            "12600193012446df9fc56919263e851c",
            "e3474cb1f5fc47b7ab8bea5b2d8b9f13",
            "9b75969f7cd44438887a1231b3359363",
            "3d53e20f22ec4392b02db75bc1f05acc",
            "2eb0c143103a464cbeba13091540f0a0",
            "261a54049e0b47d699b8ed0fba9f6407",
            "7ffd0c57fb364754b79f42a16a4b4f9f",
            "6d3837c30a3f4a36b1bc6dbdf8d8b202",
            "0f61c1680c854f2098685ad0c2dac7fb",
            "18b35624ef3a467d94f3108f821c9f72",
            "2c814504369a413ab0d79758f9064cbb",
            "4eda95f6afc3498bb06ffa2a2d5ba8ac",
            "8046df5d41e943e09e00cb8c0d94ee2b",
            "a887a414e5c04de6938d96f1bbd58518",
            "385c3952442f4281a9e7a2c2c62e57c4",
            "b3ee5e7316c84db78677ed26eb169cdf",
            "fa579c1b42cd4c2599a398e6ae123600",
            "b28975e75ae949fe9dc028c5817836cb",
            "d626d3d44d454962b51b21afd0081575",
            "f04039f83ecc49ec841030347a9f4637",
            "68de03ffd25b43748ced93f82f7b97bb",
            "2b4771a844844a43a54c8a23d247ad2f",
            "5e25285f0c3042eb8815b7dc236d97e9",
            "8f16cc5b123d4a44bd3110b05fd19c8d",
            "a670bfef711a4db587d972e4625b05cc",
            "4ae97bba358944f09d9b97a6adc33843",
            "91a493a0381943c489c4331f01be2590",
            "6a97d8451b114a6dbe4599c8b9306709",
            "11940a604b8e415683030ca0646848f6",
            "4ce0587c27284131a3203dd5917cc1f9",
            "6cd42932f62e4dcb83f8a72f59ba15b9",
            "7ac97d2863424b8a847b5b234f3e64b6",
            "d63c467c7574450995b03335fbbfa338",
            "f7756d14029f4a4b96aef0b38093bc77",
            "aa1da0195f4045b994cba00b6799d0b6",
            "0307db116d594941b125dce1fd051eba",
            "3feed8a403a54f608ee121a0221259bb",
            "0dc03f86fdc043e3a1636c0a7ceda870",
            "7b135f4e14f849c6bcf91a4f6cf8e697",
            "eacdb7481158433c8e5a91e25466b306",
            "68b4b87cbda74c97a763f07d4117e38b",
            "6d76032043f14ba5ab7fd6c7822e0928",
            "7b6f41ace242485bbbb31ad94523c6b9",
            "f4aaf08dcdc6480ca1b1508079fc574a",
            "285b852b6aa248399e2aace17d378a64",
            "dd8ba0dde6ff439a95b00b8053bb79d0",
            "c7df3b8ad03547ca88916c90ce783542",
            "70587a582a874c66abcea112e91fd89b",
            "1133d959130d445c9f4618454412de55",
            "f3b691d8cb8949f18b4fdce7e571ae39",
            "0b9c32b8b03d446380df43e781d37348",
            "f07b22e8b5b6469b9d40e541843191cf",
            "a080b61b6e8f499eb88477588d2e722e",
            "e03b9777cc514aeaa86afca14aec7fc8",
            "8ea500e9906d45b2ad186bf167ed4032",
            "075c6605577647ddbe7358160284a981",
            "a5255e2ebd4345b3ae98478170254025",
            "fa275517739e4177a620904c8515c158",
            "417a4eaad1b3412193bf13d5fdeb2edc",
            "26b84ba86eb2401b90fb21078ae406cb",
            "5aedd35ef5d34a1ca05236a64a892eeb",
            "25c795cac42d406a892b861ed09db523",
            "7e7e53b51e604d789fc1170a3b2a63c6",
            "6771204c35b74c27a6ac6e9a7e4ae119",
            "7a64e2d0ba4e4e68a723464897480baa",
            "d7d81b21f7b74f3d97ed84858291f5a9",
            "8a52902825744a63a1f5360b6b7075e5",
            "9f59a5f4d93b4948a0efe74e01bf835e",
            "b661e749fa184c4cac592a7aaa90b744",
            "555b8f977fa34e5eaad7de5fc3c29412",
            "bafc58cd4c714bdd82912ced0008e4fb",
            "35cdc32d322b4bedb256efd8be7fc526",
            "cae27eaef35142f4900e32a7495ce576",
            "1cee50f357554f66a46be92a1ecb45bd",
            "594decfbd6e0431aace64adf5c1f8cdd",
            "424a44debdba4509ae1784eb3083c1e3",
            "5aad2c1f99ee40ac9a86fd56abe2c31f"
          ]
        },
        "outputId": "0171908f-5b35-41c3-8943-8158db2e5b1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff4e0766705547a3bd50949f7b6e76e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fe5620b60304bbc9359aa500d30d365"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33aefd46a1e14da1815db0fa8579d621"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1ed57d08a6548639b17497df95a9949"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6eeae078508403c80b74eccb8617ef6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2507e138792a40f59ac80d1eb123c836"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12600193012446df9fc56919263e851c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4eda95f6afc3498bb06ffa2a2d5ba8ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e25285f0c3042eb8815b7dc236d97e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7756d14029f4a4b96aef0b38093bc77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "285b852b6aa248399e2aace17d378a64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "075c6605577647ddbe7358160284a981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a52902825744a63a1f5360b6b7075e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              model                                       best_matches  \\\n",
            "0  all-MiniLM-L6-v2  [SET, SET, SET, SET, SET, SET, SET, SET, SET, ...   \n",
            "\n",
            "   processing_time_seconds  embedding_size  \n",
            "0                   108.58             384  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###OPCIONAL##### SOLO EJECUTAR para reahacer el analisis de proximidad con otra categoria como target sin tener que repetir los embeedings\n",
        "# el identificador [component] de la categoria que quiero mostrar en el resumen de clasificación global\n",
        "# target_category ='HIWPshortDescrip'\n",
        "# target_category ='Empowerment'\n",
        "# target_category ='WorkLifeBalance'\n",
        "# target_category ='RemoteWork'\n",
        "# target_category ='GrenHRM'\n",
        "# target_category ='OpMange'\n",
        "# target_category ='KaizenLong'\n",
        "# target_category ='KaizenCulture'\n",
        "# target_category ='promptA'\n",
        "# target_category ='SLR'\n",
        "target_category ='OpenEnded'\n",
        "# promptAscag promptAperf promptBscag promptBperfCa promptBperf promptBca\n",
        "\n",
        "# Crear la nueva tabla de similitud para la target_category\n",
        "df_hiwp_similarity = df_similarity[['Model', 'Object', target_category]].copy()\n",
        "df_hiwp_similarity.columns = ['Model', 'Object_Description', 'Similarity']\n",
        "df_hiwp_similarity['Object_Component'] = objects['Component'].tolist() * len(models_to_test)\n",
        "df_hiwp_similarity = df_hiwp_similarity[['Model', 'Object_Component', 'Object_Description', 'Similarity']]\n",
        "df_hiwp_similarity = df_hiwp_similarity.sort_values(['Model', 'Similarity'], ascending=[True, False])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zs74EAWuYLNd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula el rango de similitud para cada modelo individualmente, donde el rango 1 es el mejor (mayor similitud).\n",
        "# Calcula el rango promedio para cada Object_Description a través de todos los modelos.\n",
        "# Ordena los objetos por su rango promedio (el más bajo es el mejor).\n",
        "# La tabla resumen final contendrá las siguientes columnas:\n",
        "# # Object_Component: El componente del objeto.\n",
        "# # Object_Description: La descripción del objeto.\n",
        "# # Average_Rank: El rango promedio del objeto a través de todos los modelos (más bajo es mejor).\n",
        "# Los objetos estarán ordenados por su rango promedio,\n",
        "# lo que  dará una visión integrada de cómo se comportan los objetos a clasificar, a través de todos los modelos,\n",
        "# en relación con la categoria sobre la que me interesa clasificarlos.\n",
        "\n",
        "# No es sencillo poner un punto de corte a partir del cual se consideran \"excluidos\" los objetos a clasificar.\n",
        "# de modo que se haría un escreening por titulo y abstract manual.\n",
        "# partiendo del orden de esta tabla hasta llegar aun  punto donde # haya varios seguidos sin seleccionar.\n",
        "# Momento en el que se podría parar el screening asistido por clasificacion automática.\n",
        "\n",
        "# Función para calcular el rango inverso (el más alto obtiene el rango 1)\n",
        "def inverse_rank(series):\n",
        "    return series.rank(ascending=False, method='min')\n",
        "\n",
        "# Calcular rangos para cada modelo\n",
        "df_ranks = df_hiwp_similarity.groupby('Model').apply(lambda x: x.assign(Rank=inverse_rank(x['Similarity']))).reset_index(drop=True)\n",
        "\n",
        "# Calcular el rango promedio para cada Object_Component a través de todos los modelos\n",
        "df_summary = df_ranks.groupby('Object_Component')['Rank'].mean().reset_index()\n",
        "df_summary = df_summary.rename(columns={'Rank': 'Average_Rank'})\n",
        "\n",
        "# Añadir la Description correspondiente a cada Component\n",
        "df_summary = df_summary.merge(objects[['Component', 'Description']], left_on='Object_Component', right_on='Component', how='left')\n",
        "\n",
        "# Ordenar por rango promedio (ascendente, ya que el rango más bajo es mejor)\n",
        "df_summary = df_summary.sort_values('Average_Rank')\n",
        "\n",
        "# Reordenar las columnas\n",
        "df_summary = df_summary[['Object_Component', 'Description', 'Average_Rank']]\n",
        "\n",
        "# Mostrar las primeras filas de la tabla resumen\n",
        "print(target_category)\n",
        "print(df_summary.head(20))"
      ],
      "metadata": {
        "id": "AFNE3e7c6NAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b63bf8d-f9e5-45f4-851f-97c32f728586"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenEnded\n",
            "                              Object_Component  \\\n",
            "690         id0926--10.1027/2151-2604/a000566#   \n",
            "443                 id0631--10.3926/jiem.5620#   \n",
            "863               id1127--10.3390/app12010514#   \n",
            "241     id0400--10.1080/14703297.2020.1810099#   \n",
            "220     id0378--10.1080/07294360.2021.1967887#   \n",
            "557         id0770--10.1007/s10755-015-9328-5#   \n",
            "924       id1256--10.1109/ACCESS.2021.3116425#   \n",
            "612       id0838--10.1109/ACCESS.2023.3305260#   \n",
            "125     id0270--10.1080/02602938.2023.2199486#   \n",
            "331     id0498--10.1080/02602938.2019.1614524#   \n",
            "722      id0960--10.1007/978-3-030-40160-3_14#   \n",
            "189                  id0345--10.5204/ssj.2756#   \n",
            "675                                  id0910--#   \n",
            "569             id0787--10.14507/epaa.29.6289#   \n",
            "384     id0558--10.1016/j.stueduc.2019.02.004#   \n",
            "820                id1070--10.5209/RCED.59224#   \n",
            "466  id0658--10.1111/j.1467-842X.2005.00414.x#   \n",
            "685     id0920--10.1016/j.compedu.2020.103965#   \n",
            "643                 id0873--10.1002/bmb.20917#   \n",
            "571      id0789--10.1080/02602938.2010.494403#   \n",
            "\n",
            "                                           Description  Average_Rank  \n",
            "690  Improving and Analyzing Open-Ended Survey Resp...           1.0  \n",
            "443  Effectiveness of the Use of Open-Ended Questio...           2.0  \n",
            "863  Leveraging AI and Machine Learning for Nationa...           3.0  \n",
            "241  What are they trying to tell me? Large-scale v...           4.0  \n",
            "220  Contribution of open-ended questions in studen...           5.0  \n",
            "557  Student Evaluation of Teaching: A Study Explor...           6.0  \n",
            "924  Palaute: An Online Text Mining Tool for Analyz...           7.0  \n",
            "612  Summarizing Students' Free Responses for an In...           8.0  \n",
            "125  Student motivations, perceptions and opinions ...           9.0  \n",
            "331  Advancing text-analysis to tap into the studen...          10.0  \n",
            "722  Student Evaluations of Teaching at the Univers...          11.0  \n",
            "189  Allegations, Abuse and Discrimination: Using S...          12.0  \n",
            "675  A Suite of Google Services for Daily Course Ev...          13.0  \n",
            "569  The Value of Student Feedback in Open Forums: ...          14.0  \n",
            "384  Standardised module evaluation surveys in UK h...          15.0  \n",
            "820  Benefits in the distribution of evaluation of ...          16.0  \n",
            "466  The Dogit ordered generalized extreme value mo...          17.0  \n",
            "685  Improving the quality of teaching by utilising...          18.0  \n",
            "643  Acceptance of Clickers in a Large Multimodal B...          19.0  \n",
            "571  Reining in student comments: a model for categ...          20.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-20-4037176016.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_ranks = df_hiwp_similarity.groupby('Model').apply(lambda x: x.assign(Rank=inverse_rank(x['Similarity']))).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# etiquetar los archivos con alguna información adcional\n",
        "# tag_file = tag_file\n",
        "# tag_file = 'best5 models'\n",
        "# tag_file = 'worst9 models'\n",
        "# tag_file = ''\n",
        "# tag_file = 'all models-articles'\n",
        "tag_file = 'UBschoolMay25-UnModelo_all-MiniLM-L6-v2'"
      ],
      "metadata": {
        "id": "ECVs3u4GYfMV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# código para guardar todos los DataFrames en un único archivo Excel, con cada DataFrame en una hoja separada\n",
        "\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "\n",
        "# Asegurarse de que la carpeta de salida existe\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Crear el nombre del archivo Excel\n",
        "excel_filename = f'classification_{tag_file}_{target_category}_{current_time}.xlsx'\n",
        "excel_path = os.path.join(output_path, excel_filename)\n",
        "\n",
        "# Crear un ExcelWriter object\n",
        "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "    # Guardar cada DataFrame en una hoja separada\n",
        "    df_results.to_excel(writer, sheet_name='model_comparison_results', index=False)\n",
        "    df_similarity.to_excel(writer, sheet_name='tablas_similitud_coseno', index=False)\n",
        "    df_embeddings.to_excel(writer, sheet_name='embeddings', index=False)\n",
        "    df_hiwp_similarity.to_excel(writer, sheet_name='hiwp_similarity', index=False)\n",
        "    df_summary.to_excel(writer, sheet_name='hiwp_similarity_summary', index=False)\n",
        "\n",
        "print(f\"Todos los resultados han sido guardados en {excel_path}\")"
      ],
      "metadata": {
        "id": "A2-Jj3-GrwW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8151087-d4aa-40c5-ec01-20b9b207febd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todos los resultados han sido guardados en /content/classification_UBschoolMay25-UnModelo_all-MiniLM-L6-v2_promptA_20250702_220920.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t-dRK7z2hBD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPCIONAL. un código que lea los embeddings del archivo Excel y los prepare para poder reutilizarlos con los chunks de codigo de...\n",
        "\n",
        "ACEDE-ECN-dic24-cribado_LLMhiwp_embeddingsv2-shared.ipynb\n",
        "(basado en ACEDE_ECN_dic24_cribado_LLMhiwp_embeddingsv2_shared.ipynb)\n",
        "\n",
        "Sin tnere que volver a calcular los embeddigns, que según el numero de objeto y modelo puedes consumir mucho tiempo (por ejemplo con 1024 tolens y 1200 objetos a vectorizar son unos 30 minutos, con un modelo de 384 tokesn son unos 3 minutos solamente)\n",
        "\n",
        "El dato de entrada datosembeddings.xlsx es el resultdo que guarda cualquiera de esos dos códigos\n",
        "\n",
        "TEngo que terener cargados los dataframe de objects y categories..."
      ],
      "metadata": {
        "id": "dS9bY3CiXwi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyNpavQ5XjQT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import util\n",
        "import ast\n",
        "\n",
        "# Leer el archivo Excel\n",
        "# Para archivo .xls\n",
        "# df_embeddings = pd.read_excel('datosembeddings.xls', sheet_name='embeddings', engine='xlrd')\n",
        "\n",
        "# Para archivo .xlsx\n",
        "df_embeddings = pd.read_excel('ART-749-embeddings.xlsx', sheet_name='embeddings', engine='openpyxl')\n",
        "\n",
        "# Obtener la lista de modelos únicos del DataFrame de embeddings\n",
        "models_to_test = df_embeddings['Model'].unique().tolist()\n",
        "\n",
        "# Convertir los embeddings de string a lista de números\n",
        "df_embeddings['Embedding'] = df_embeddings['Embedding'].apply(ast.literal_eval)\n",
        "\n",
        "# Crear diccionario para almacenar los resultados\n",
        "results = []\n",
        "similarity_data = []\n",
        "\n",
        "# Procesar cada modelo\n",
        "for model_name in df_embeddings['Model'].unique():\n",
        "    # Filtrar datos para el modelo actual\n",
        "    model_data = df_embeddings[df_embeddings['Model'] == model_name]\n",
        "\n",
        "    # Separar embeddings de objetos y categorías\n",
        "    objects_data = model_data[model_data['Component'].isin(objects['Component'])]\n",
        "    categories_data = model_data[model_data['Component'].isin(categories['Component'])]\n",
        "\n",
        "    # Convertir embeddings a arrays numpy\n",
        "    object_embeddings = np.array(objects_data['Embedding'].tolist())\n",
        "    category_embeddings = np.array(categories_data['Embedding'].tolist())\n",
        "\n",
        "    # Calcular similitud coseno\n",
        "    similarity_matrix = util.cos_sim(object_embeddings, category_embeddings)\n",
        "\n",
        "    # Encontrar las mejores coincidencias\n",
        "    best_matches = np.argmax(similarity_matrix, axis=1)\n",
        "\n",
        "    # Guardar resultados\n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'best_matches': categories.iloc[best_matches]['Component'].tolist(),\n",
        "        'embedding_size': object_embeddings.shape[1]\n",
        "    })\n",
        "\n",
        "    # Preparar datos de similitud para este modelo\n",
        "    for i, obj in enumerate(objects['Component']):\n",
        "        row = [model_name, obj] + similarity_matrix[i].tolist()\n",
        "        similarity_data.append(row)\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Crear DataFrame de similitud coseno\n",
        "columns = ['Model', 'Object'] + categories['Component'].tolist()\n",
        "df_similarity = pd.DataFrame(similarity_data, columns=columns)\n",
        "\n",
        "# A partir de aquí puedes usar el código que ya tenías para crear df_hiwp_similarity\n",
        "df_hiwp_similarity = df_similarity[['Model', 'Object', target_category]].copy()\n",
        "df_hiwp_similarity.columns = ['Model', 'Object_Description', 'Similarity']\n",
        "df_hiwp_similarity['Object_Component'] = objects['Component'].tolist() * len(models_to_test)\n",
        "df_hiwp_similarity = df_hiwp_similarity[['Model', 'Object_Component', 'Object_Description', 'Similarity']]\n",
        "df_hiwp_similarity = df_hiwp_similarity.sort_values(['Model', 'Similarity'], ascending=[True, False])\n",
        "\n",
        "# continua con ###OPCIONAL##### SOLO EJECUTAR para reahacer el analisis de proximidad con otra categoria como target sin tener que repetir los embeedings\n",
        "# o con # Calcula el rango de similitud para cada modelo individualmente, donde el rango 1 es el mejor (mayor similitud).\n"
      ]
    }
  ]
}
